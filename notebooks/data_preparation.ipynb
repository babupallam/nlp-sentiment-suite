{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3dc38aee24b15ec8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 1: Import Libraries and Define Paths\n",
    "\n"
   ],
   "id": "a2764d990df88391"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:31:04.814599Z",
     "start_time": "2024-11-26T22:31:04.789931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import contractions\n",
    "import logging\n",
    "\n",
    "# Setup for NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Set paths for raw and processed data\n",
    "data_path = \"../data/\"\n",
    "raw_data_file = os.path.join(data_path, \"Twitter_Data.csv\")\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(filename='data_preprocessing.log', level=logging.INFO)\n",
    "\n",
    "# Define Emoji and Stopword Information\n",
    "emoji_dict = {\"üòä\": \"happy\", \"üò¢\": \"sad\", \"‚ù§Ô∏è\": \"love\"}\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ],
   "id": "88723fd196013b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Girija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Girija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Girija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 2: Analyze Dataset\n",
    "\n"
   ],
   "id": "71cdadca55d23a50"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:31:05.253325Z",
     "start_time": "2024-11-26T22:31:04.845647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def analyze_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Function to analyze the dataset structure to check for its size, columns, data types, \n",
    "    and missing values to get an overview of the data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"\\n--- Dataset Analysis ---\")\n",
    "        print(f\"Dataset Shape: {df.shape}\")  # Display rows and columns\n",
    "        print(\"\\nColumns and Data Types:\")\n",
    "        print(df.dtypes)  # Check data types of each column\n",
    "        print(\"\\nFirst Few Rows:\")\n",
    "        print(df.head())  # Show a preview of the dataset\n",
    "        print(\"\\nMissing Values Count:\")\n",
    "        print(df.isnull().sum())  # Show number of missing values per column\n",
    "        logging.info(f\"Dataset loaded and analyzed: Shape - {df.shape}, Columns - {list(df.columns)}\")\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {file_path}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Analyze the dataset\n",
    "df_analysis = analyze_dataset(raw_data_file)\n",
    "if df_analysis is None:\n",
    "    exit()  # Exit if the dataset file is not found\n"
   ],
   "id": "9435cefbee192a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset Analysis ---\n",
      "Dataset Shape: (162980, 2)\n",
      "\n",
      "Columns and Data Types:\n",
      "clean_text     object\n",
      "category      float64\n",
      "dtype: object\n",
      "\n",
      "First Few Rows:\n",
      "                                          clean_text  category\n",
      "0  when modi promised ‚Äúminimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n",
      "\n",
      "Missing Values Count:\n",
      "clean_text    4\n",
      "category      7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 3: Load Dataset\n",
    "\n",
    "**Why**: Load the raw dataset into a DataFrame so it can be used for processing.\n",
    "\n"
   ],
   "id": "439b836b1c9c1ccd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:31:05.708295Z",
     "start_time": "2024-11-26T22:31:05.288317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Function to load the dataset into a DataFrame. Logs errors if the dataset file is missing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "        logging.info(f\"Dataset loaded successfully with shape: {df.shape}\")\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {file_path}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "df = load_dataset(raw_data_file)\n",
    "if df is None:\n",
    "    exit()  # Exit if the dataset couldn't be loaded\n"
   ],
   "id": "a73d9b89c5e4a0aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with shape: (162980, 2)\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 4: Handle Contractions and Preprocess Text Data\n",
    "\n",
    "**Why**: Expand contractions to ensure uniformity in the text (e.g., \"don't\" ‚Üí \"do not\"), which can improve tokenization and overall model performance.\n",
    "\n"
   ],
   "id": "406a2de1691c7c26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:31:07.265941Z",
     "start_time": "2024-11-26T22:31:05.740766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Function to expand contractions in the text for standardization.\n",
    "    If the input is not a string (e.g., NaN), return an empty string.\n",
    "    \"\"\"\n",
    "    if isinstance(text, str):\n",
    "        return contractions.fix(text)\n",
    "    else:\n",
    "        return \"\"  # or you could return `text` to keep NaNs as they are\n",
    "\n",
    "# Apply contraction expansion to the dataset's text column\n",
    "df['clean_text'] = df['clean_text'].apply(expand_contractions)\n"
   ],
   "id": "5a2e7c008ad5d659",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 5: Text Normalization\n",
    "\n",
    "**Why**: Normalize the text data by removing unnecessary characters, URLs, hashtags, mentions, and converting to lowercase for uniformity.\n",
    "\n"
   ],
   "id": "fe6d518055069c3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:31:09.583106Z",
     "start_time": "2024-11-26T22:31:07.299683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Function to normalize text by converting to lowercase, removing URLs, \n",
    "    mentions, hashtags, special characters, and expanding emojis.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Replace emojis with words\n",
    "    for emoji, word in emoji_dict.items():\n",
    "        text = text.replace(emoji, word)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply text normalization\n",
    "df['clean_text'] = df['clean_text'].apply(normalize_text)\n"
   ],
   "id": "722266593b3d6d4e",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 6: Tokenization\n",
    "\n",
    "**Why**: Tokenize text into individual words to facilitate further text processing.\n",
    "\n"
   ],
   "id": "e6a438234366c94e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:31:28.919675Z",
     "start_time": "2024-11-26T22:31:09.614497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "nltk.download('punkt_tab')\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Function to tokenize text into individual words (tokens).\n",
    "    \"\"\"\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Apply tokenization\n",
    "df['tokens'] = df['clean_text'].apply(tokenize_text)\n"
   ],
   "id": "547ee2be2eb1f368",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Girija\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 7: Remove Noise - Stop Words and Short Tokens\n",
    "\n",
    "**Why**: Remove stop words (e.g., \"is\", \"the\") that don‚Äôt add significant meaning and remove short or meaningless tokens.\n",
    "\n"
   ],
   "id": "e789880ddd012547"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:31:30.092337Z",
     "start_time": "2024-11-26T22:31:28.984045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Function to remove stopwords and very short tokens that don't add significant meaning.\n",
    "    \"\"\"\n",
    "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "\n",
    "# Apply stopword removal\n",
    "df['tokens'] = df['tokens'].apply(remove_stopwords)\n",
    "print(df)\n"
   ],
   "id": "c5e6d62b3fcc97c0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               clean_text  category  \\\n",
      "0       when modi promised minimum government maximum ...      -1.0   \n",
      "1       talk all the nonsense and continue all the dra...       0.0   \n",
      "2       what did just say vote for modi welcome bjp to...       1.0   \n",
      "3       asking his supporters prefix chowkidar their n...       1.0   \n",
      "4       answer who among these the most powerful world...       1.0   \n",
      "...                                                   ...       ...   \n",
      "162975  why these crores paid neerav modi not recovere...      -1.0   \n",
      "162976  dear rss terrorist payal gawar what about modi...      -1.0   \n",
      "162977  did you cover her interaction forum where she ...       0.0   \n",
      "162978  there big project came into india modi dream p...       0.0   \n",
      "162979  have you ever listen about like gurukul where ...       1.0   \n",
      "\n",
      "                                                   tokens  \n",
      "0       [modi, promised, minimum, government, maximum,...  \n",
      "1           [talk, nonsense, continue, drama, vote, modi]  \n",
      "2       [say, vote, modi, welcome, bjp, told, rahul, m...  \n",
      "3       [asking, supporters, prefix, chowkidar, names,...  \n",
      "4       [answer, among, powerful, world, leader, today...  \n",
      "...                                                   ...  \n",
      "162975  [crores, paid, neerav, modi, recovered, congre...  \n",
      "162976  [dear, rss, terrorist, payal, gawar, modi, kil...  \n",
      "162977                  [cover, interaction, forum, left]  \n",
      "162978  [big, project, came, india, modi, dream, proje...  \n",
      "162979  [ever, listen, like, gurukul, discipline, main...  \n",
      "\n",
      "[162980 rows x 3 columns]\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 8: Lemmatization\n",
    "\n",
    "**Why**: Lemmatize words to reduce them to their base form (e.g., \"running\" ‚Üí \"run\") for consistent representation.\n",
    "\n"
   ],
   "id": "ed674254089e02d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:31:38.812735Z",
     "start_time": "2024-11-26T22:31:30.125721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Function to lemmatize tokens to get the base form of words.\n",
    "    \"\"\"\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "df['tokens'] = df['tokens'].apply(lemmatize_tokens)\n"
   ],
   "id": "a163994cc99cb14c",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 9: Feature Engineering\n",
    "\n",
    "**Why**: Add new features to help the model understand text better, such as text length, word count, and sentiment scores.\n",
    "\n"
   ],
   "id": "2377abd053111f9c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:32:13.128166Z",
     "start_time": "2024-11-26T22:31:38.864184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    \"\"\"\n",
    "    Function to get sentiment score using TextBlob.\n",
    "    \"\"\"\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Adding text length and word count as features\n",
    "df['text_length'] = df['clean_text'].apply(len)\n",
    "df['word_count'] = df['tokens'].apply(len)\n",
    "\n",
    "# Adding sentiment score as a feature\n",
    "df['sentiment_score'] = df['clean_text'].apply(get_sentiment_score)\n"
   ],
   "id": "54e8fa9565e66a1f",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 10: Validate and Balance Dataset\n",
    "\n",
    "**Why**: Ensure the data is balanced. If there is a significant imbalance, apply methods to handle it, such as using SMOTE.\n",
    "\n",
    "    - which is commonly used for handling imbalanced datasets, particularly through techniques like SMOTE (Synthetic Minority Oversampling Technique)\n",
    "    - SMOTE creates synthetic examples of the minority class by interpolating between existing minority samples, which helps balance the dataset and improve model training.\n",
    "\n",
    "\n",
    "- **Check for Label Balance**:\n",
    "    - **Imbalanced Dataset**: Detect and balance classes to ensure the model doesn‚Äôt favor a dominant class.\n",
    "\n"
   ],
   "id": "79ba4dea1fe0a51"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:32:13.191979Z",
     "start_time": "2024-11-26T22:32:13.162656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def validate_and_handle_imbalance(df):\n",
    "    \"\"\"\n",
    "    Function to validate label balance and handle class imbalance using SMOTE.\n",
    "    \"\"\"\n",
    "    # Display unique labels in 'category'\n",
    "    print(\"Unique categories:\", df['category'].unique())\n",
    "    label_counts = df['category'].value_counts()\n",
    "    print(\"Label distribution:\", label_counts)\n",
    "\n",
    "    # Check for imbalance and apply SMOTE if necessary\n",
    "    if label_counts.min() < 0.1 * label_counts.max():\n",
    "        print(\"Warning: Significant imbalance detected. Applying SMOTE to handle imbalance.\")\n",
    "        smote = SMOTE(random_state=42)\n",
    "        # Resample text and labels using TF-IDF vectorized form to maintain balance\n",
    "        vectorizer_tfidf = TfidfVectorizer()\n",
    "        X = vectorizer_tfidf.fit_transform(df['clean_text'])\n",
    "        y = df['category']\n",
    "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "        return X_resampled, y_resampled, vectorizer_tfidf\n",
    "    else:\n",
    "        # Return original if no significant imbalance is found\n",
    "        return None, None, None\n",
    "\n",
    "# Apply label validation and handling imbalance if required\n",
    "X_resampled, y_resampled, tfidf_vectorizer = validate_and_handle_imbalance(df)\n"
   ],
   "id": "e800bf350573ee8e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories: [-1.  0.  1. nan]\n",
      "Label distribution: category\n",
      " 1.0    72250\n",
      " 0.0    55213\n",
      "-1.0    35510\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 11: Split Dataset\n",
    "\n",
    "**Why**: Divide the data into training, validation, and test sets to properly train and evaluate the model without overfitting.\n",
    "\n",
    "- **Training Set**: Used to train the model.\n",
    "- **Validation Set**: Used to tune the model and check its performance during training.\n",
    "- **Test Set**: Used to evaluate the model's final performance.\n",
    "\n"
   ],
   "id": "f266f95eac725ca1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:32:13.647946Z",
     "start_time": "2024-11-26T22:32:13.225375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Drop rows with NaN values in the target variable ('category')\n",
    "df = df.dropna(subset=['category'])\n",
    "\n",
    "# Step 2: Convert 'category' to integers if applicable\n",
    "df.loc[:, 'category'] = df['category'].astype(int)\n",
    "\n",
    "# Step 3: Split the Dataset\n",
    "def split_dataset(df):\n",
    "    \"\"\"\n",
    "    Function to split the dataset into training, validation, and test sets.\n",
    "    Uses stratified sampling to ensure balanced distribution of categories in each set.\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['category'])\n",
    "    train, val = train_test_split(train, test_size=0.2, random_state=42, stratify=train['category'])\n",
    "    print(f\"Training set size: {train.shape}\")\n",
    "    print(f\"Validation set size: {val.shape}\")\n",
    "    print(f\"Test set size: {test.shape}\")\n",
    "    return train, val, test\n",
    "\n",
    "# Apply split function\n",
    "train_df, val_df, test_df = split_dataset(df)\n"
   ],
   "id": "ceefdb5f5f5ca590",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (104302, 6)\n",
      "Validation set size: (26076, 6)\n",
      "Test set size: (32595, 6)\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 12: Numerical Transformation\n",
    "\n",
    "**Why**: Machine learning models need numerical inputs. We need to convert the cleaned text into numerical formats using different methods such as Bag of Words (BoW), TF-IDF, and Word Embeddings.\n",
    "\n",
    "##### **Bag of Words (BoW) Representation**\n",
    "\n",
    "- Converts text into a vector of word frequencies.\n",
    "- **Training**: Fit on training data.\n",
    "- **Validation and Test**: Transform using the same vectorizer.\n",
    "\n"
   ],
   "id": "19e0be1375a833cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:34:43.071786Z",
     "start_time": "2024-11-26T22:34:39.070816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Bag of Words Representation\n",
    "print(\"Generating Bag of Words (BoW) representations...\\n\")\n",
    "\n",
    "# Step 1: Fit the Count Vectorizer\n",
    "vectorizer_bow = CountVectorizer()\n",
    "X_train_bow = vectorizer_bow.fit_transform(train_df['clean_text'])\n",
    "\n",
    "# Step 2: Transform Validation and Test Sets\n",
    "X_val_bow = vectorizer_bow.transform(val_df['clean_text'])\n",
    "X_test_bow = vectorizer_bow.transform(test_df['clean_text'])\n",
    "\n",
    "# Demonstration of CountVectorizer Output\n",
    "print(\"Bag of Words Transformation Completed!\\n\")\n",
    "\n",
    "# Print the number of features created (size of the vocabulary)\n",
    "print(f\"Number of features (vocabulary size): {len(vectorizer_bow.get_feature_names_out())}\")\n",
    "\n",
    "# Step 3: Display Sample Features and Their Count Scores\n",
    "# Select the first sample from the training set for demonstration\n",
    "sample_index = 0\n",
    "sample_text = train_df['clean_text'].iloc[sample_index]\n",
    "bow_counts = X_train_bow[sample_index]\n",
    "\n",
    "# Convert BoW sparse matrix to a dense array and match with feature names\n",
    "feature_names = vectorizer_bow.get_feature_names_out()\n",
    "dense_bow = bow_counts.toarray().flatten()\n",
    "\n",
    "# Create a DataFrame with words and their count scores\n",
    "bow_data = pd.DataFrame({'Word': feature_names, 'Count': dense_bow})\n",
    "top_bow_data = bow_data[bow_data['Count'] > 0].sort_values(by='Count', ascending=False).head(10)\n",
    "\n",
    "# Step 4: Print out a demonstration of the Bag of Words features for the first sample\n",
    "print(\"\\nExample Text (Training Sample):\")\n",
    "print(sample_text)\n",
    "print(\"\\nTop 10 Words with Highest Counts in the Example Text:\")\n",
    "print(top_bow_data)\n"
   ],
   "id": "459a88e40d24a56c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Bag of Words (BoW) representations...\n",
      "\n",
      "Bag of Words Transformation Completed!\n",
      "\n",
      "Number of features (vocabulary size): 76876\n",
      "\n",
      "Example Text (Training Sample):\n",
      "galaxy bjp leaders led modi and party chief amit shah will hit the campaign trail telangana\n",
      "\n",
      "Top 10 Words with Highest Counts in the Example Text:\n",
      "           Word  Count\n",
      "2591       amit      1\n",
      "2808        and      1\n",
      "8320        bjp      1\n",
      "10399  campaign      1\n",
      "12075     chief      1\n",
      "25771    galaxy      1\n",
      "30219       hit      1\n",
      "38741   leaders      1\n",
      "38885       led      1\n",
      "43774      modi      1\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### **TF-IDF (Term Frequency-Inverse Document Frequency) Representation**\n",
    "\n",
    "- Weights the importance of each word based on how often it appears across all documents.\n",
    "- Helps in reducing the impact of common words that appear frequently but may not be informative.\n",
    "\n"
   ],
   "id": "da695e7a2cccb5be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:33:40.780901Z",
     "start_time": "2024-11-26T22:33:36.390368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# TF-IDF Representation\n",
    "print(\"Generating TF-IDF representations...\\n\")\n",
    "\n",
    "# Step 1: Fit the TF-IDF Vectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(train_df['clean_text'])\n",
    "\n",
    "# Step 2: Transform Validation and Test Sets\n",
    "X_val_tfidf = vectorizer_tfidf.transform(val_df['clean_text'])\n",
    "X_test_tfidf = vectorizer_tfidf.transform(test_df['clean_text'])\n",
    "\n",
    "# Demonstration of TF-IDF Vectorizer Output\n",
    "print(\"TF-IDF Transformation Completed!\\n\")\n",
    "\n",
    "# Print the number of features created (size of the vocabulary)\n",
    "print(f\"Number of features (vocabulary size): {len(vectorizer_tfidf.get_feature_names_out())}\")\n",
    "\n",
    "# Step 3: Display Sample Features and Their TF-IDF Scores\n",
    "# Select the first sample from the training set for demonstration\n",
    "sample_index = 0\n",
    "sample_text = train_df['clean_text'].iloc[sample_index]\n",
    "tfidf_scores = X_train_tfidf[sample_index]\n",
    "\n",
    "# Convert TF-IDF sparse matrix to a dense array and match with feature names\n",
    "feature_names = vectorizer_tfidf.get_feature_names_out()\n",
    "dense_tfidf = tfidf_scores.toarray().flatten()\n",
    "\n",
    "# Create a DataFrame with words and their TF-IDF scores\n",
    "tfidf_data = pd.DataFrame({'Word': feature_names, 'TF-IDF Score': dense_tfidf})\n",
    "top_tfidf_data = tfidf_data[tfidf_data['TF-IDF Score'] > 0].sort_values(by='TF-IDF Score', ascending=False).head(10)\n",
    "\n",
    "# Step 4: Print out a demonstration of the TF-IDF features for the first sample\n",
    "print(\"\\nExample Text (Training Sample):\")\n",
    "print(sample_text)\n",
    "print(\"\\nTop 10 Words with Highest TF-IDF Scores in the Example Text:\")\n",
    "print(top_tfidf_data)\n",
    "\n"
   ],
   "id": "e16399e7aff27292",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TF-IDF representations...\n",
      "\n",
      "TF-IDF Transformation Completed!\n",
      "\n",
      "Number of features (vocabulary size): 76876\n",
      "\n",
      "Example Text (Training Sample):\n",
      "galaxy bjp leaders led modi and party chief amit shah will hit the campaign trail telangana\n",
      "\n",
      "Top 10 Words with Highest TF-IDF Scores in the Example Text:\n",
      "            Word  TF-IDF Score\n",
      "25771     galaxy      0.456295\n",
      "70127      trail      0.387074\n",
      "67973  telangana      0.317364\n",
      "30219        hit      0.284731\n",
      "38885        led      0.283387\n",
      "2591        amit      0.251792\n",
      "12075      chief      0.250234\n",
      "38741    leaders      0.232948\n",
      "62025       shah      0.228080\n",
      "10399   campaign      0.226465\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Step 13: Save Processed Data\n",
    "\n",
    "**Why**: Save the cleaned and transformed datasets for reuse. Saving the numerical representations like BoW and TF-IDF helps speed up the process during model training.\n",
    "\n",
    "##### **Save Cleaned and Split Datasets**\n",
    "\n"
   ],
   "id": "38bdb121ff3aabb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T22:32:23.968931Z",
     "start_time": "2024-11-26T22:32:20.064612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "def save_datasets(dataframes, filenames, data_path):\n",
    "    \"\"\"\n",
    "    Function to save the processed datasets as CSV files for future use.\n",
    "    \"\"\"\n",
    "    for df, filename in zip(dataframes, filenames):\n",
    "        file_path = os.path.join(data_path, filename)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"Saved {filename} to {data_path}\")\n",
    "        logging.info(f\"Saved {filename} to {data_path}\")\n",
    "\n",
    "# Save datasets: Cleaned, Train, Validation, and Test\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "save_datasets(\n",
    "    [df, train_df, val_df, test_df],\n",
    "    [\"processed_data.csv\", \"train_data.csv\", \"val_data.csv\", \"test_data.csv\"],\n",
    "    data_path\n",
    ")\n"
   ],
   "id": "82ac7c628e77e4e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed_data.csv to ../data/\n",
      "Saved train_data.csv to ../data/\n",
      "Saved val_data.csv to ../data/\n",
      "Saved test_data.csv to ../data/\n"
     ]
    }
   ],
   "execution_count": 69
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
