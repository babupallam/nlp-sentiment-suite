{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define paths for loading data and saving models\n",
    "data_path = \"../data/\"\n",
    "train_file = data_path + \"train_data.csv\"\n",
    "val_file = data_path + \"val_data.csv\"\n",
    "model_save_path = \"../models/\"\n",
    "\n",
    "# Load training and validation data\n",
    "def load_data(train_path, val_path):\n",
    "    \"\"\"\n",
    "    Function to load training and validation data from CSV files.\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = load_data(train_file, val_file)\n",
    "\n",
    "# Prepare text and labels\n",
    "X_train = train_df['clean_text']\n",
    "y_train = train_df['category']\n",
    "X_val = val_df['clean_text']\n",
    "y_val = val_df['category']\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "def prepare_text_sequences(X_train, X_val, max_words=10000, max_len=100):\n",
    "    \"\"\"\n",
    "    Function to tokenize and pad text sequences for deep learning models.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    # Convert text to sequences\n",
    "    train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    val_sequences = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "    # Pad sequences\n",
    "    train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "    val_padded = pad_sequences(val_sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "    return train_padded, val_padded, tokenizer\n",
    "\n",
    "X_train_padded, X_val_padded, tokenizer = prepare_text_sequences(X_train, X_val)\n",
    "\n",
    "# One-hot encode labels\n",
    "def one_hot_encode_labels(y_train, y_val):\n",
    "    \"\"\"\n",
    "    Function to one-hot encode labels for multi-class classification.\n",
    "    \"\"\"\n",
    "    encoder = LabelBinarizer()\n",
    "    y_train_encoded = encoder.fit_transform(y_train)\n",
    "    y_val_encoded = encoder.transform(y_val)\n",
    "    return y_train_encoded, y_val_encoded, encoder\n",
    "\n",
    "y_train_encoded, y_val_encoded, label_encoder = one_hot_encode_labels(y_train, y_val)\n",
    "\n",
    "# Save the tokenizer and label encoder for reuse\n",
    "joblib.dump(tokenizer, os.path.join(model_save_path, \"tokenizer.pkl\"))\n",
    "joblib.dump(label_encoder, os.path.join(model_save_path, \"label_encoder.pkl\"))\n",
    "print(\"Tokenizer and label encoder saved.\")\n",
    "\n",
    "# Build LSTM model\n",
    "def build_lstm_model(input_dim, embedding_dim=128, input_length=100, lstm_units=64):\n",
    "    \"\"\"\n",
    "    Function to build an LSTM-based model with an embedding layer.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=input_dim, output_dim=embedding_dim, input_length=input_length),\n",
    "        Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(lstm_units // 2)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(y_train_encoded.shape[1], activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define model parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = 100\n",
    "\n",
    "lstm_model = build_lstm_model(input_dim=vocab_size, input_length=max_len)\n",
    "\n",
    "# Compile the model\n",
    "lstm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "lstm_model.fit(X_train_padded, y_train_encoded, \n",
    "               validation_data=(X_val_padded, y_val_encoded), \n",
    "               epochs=5, batch_size=64)\n",
    "\n",
    "# Save the trained model\n",
    "lstm_model.save(os.path.join(model_save_path, \"lstm_model.h5\"))\n",
    "print(\"LSTM model saved.\")\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_lstm_model(model, X_val, y_val, encoder):\n",
    "    \"\"\"\n",
    "    Function to evaluate the LSTM model on validation data.\n",
    "    \"\"\"\n",
    "    val_preds = model.predict(X_val)\n",
    "    val_preds_labels = np.argmax(val_preds, axis=1)\n",
    "    y_val_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_val_labels, val_preds_labels)\n",
    "    f1 = f1_score(y_val_labels, val_preds_labels, average='weighted')\n",
    "    precision = precision_score(y_val_labels, val_preds_labels, average='weighted')\n",
    "    recall = recall_score(y_val_labels, val_preds_labels, average='weighted')\n",
    "\n",
    "    print(\"\\nLSTM Model Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "evaluate_lstm_model(lstm_model, X_val_padded, y_val_encoded, label_encoder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
